{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/wavenet_vocoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WaveNet Vocoder Recipe Demonstration\n",
    "\n",
    "**Tomoki Hayashi**\n",
    "\n",
    "Department of Informatics, Nagoya University  \n",
    "Human Dataware Lab. Co., Ltd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environmental setup\n",
    "\n",
    "First, install dependecies (It takes several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get install -qq -y bc tree\n",
    "!git clone https://github.com/kan-bayashi/PytorchWaveNetVocoder.git -b IS19TUTORIAL\n",
    "!git clone https://github.com/k2kobayashi/sprocket.git -b IS19TUTORIAL\n",
    "!cd sprocket && pip install -q .\n",
    "!cd PytorchWaveNetVocoder && pip install -q .\n",
    "!cd PytorchWaveNetVocoder && mkdir -p tools/venv/bin && touch tools/venv/bin/activate\n",
    "import sprocket, wavenet_vocoder  # check importable\n",
    "!echo \"Setup done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the PytorchWaveNetVocoder?\n",
    "\n",
    "Github: [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder)  \n",
    "Samples: https://kan-bayashi.github.io/WaveNetVocoderSamples/\n",
    "\n",
    "- WaveNet vocoder implemention with pytorch\n",
    "- Support [kaldi](https://github.com/kaldi-asr/kaldi)-like recipes, easy to reproduce the results\n",
    "- Support [World](https://github.com/mmorise/World) features / mel-spectrogram based models\n",
    "- Support multi-gpu training / decoding\n",
    "- Support a noise shaping [[Tachibana+ 2018](https://ieeexplore.ieee.org/document/8461332)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What it the kaldi-like recipe?\n",
    "\n",
    "Key features:\n",
    "- Prepared for each corpus (e.g. CMU Arctic, LJSpeech)\n",
    "- Consists of unified several stages  \n",
    "  (e.g. data preparation, feature extraction, and so on.)\n",
    "- Includes all procedures needed to reproduce the results\n",
    "- All of the recipes are stored in `egs/<corpus>/<type>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supported corpus:\n",
    "- [CMUArctic database](http://www.festvox.org/cmu_arctic/): `egs/arctic`, 16 kHz, English, Several speakers.\n",
    "- [LJ Speech database](https://keithito.com/LJ-Speech-Dataset/): `egs/ljspeech` 22.05 kHz, English, Single female speaker.\n",
    "- [M-AILABS speech database](http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/):`egs/m-ailabs-speech`: 16 kHz, various speakers\n",
    "\n",
    "About supported type, see detail in https://github.com/kan-bayashi/PytorchWaveNetVocoder/tree/master/egs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run the demo recipe\n",
    "\n",
    "Let us run the demo recipe `egs/arctic/sd-mini`.\n",
    "\n",
    "- Small version of `egs/arctic/sd`\n",
    "- Use subset of all of the utterances\n",
    "- **Cannot build a good model** but the flow is **the same**\n",
    "\n",
    "You can understand each stage within 30 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# move on the recipe directory\n",
    "import os\n",
    "os.chdir(\"./PytorchWaveNetVocoder/egs/arctic/sd-mini\")\n",
    "!echo $(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Files in the recipe are as follows:\n",
    "- `conf`: Directory including config files\n",
    "- `path.sh`: Script to set the environmental variables.\n",
    "- `run.sh`: Main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`conf` includes f0 setting files whose name format is `<speaker_name>.f0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "!ls conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<speaker_name>.f0` includes `min_f0 max_f0`.  \n",
    "These values are predecided by ourselve, so you can modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat conf/slt.f0  # (minf0 maxf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All of the hyperparameters are written in `run.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 69 run.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let us introduce these parameters in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can add your command to check the file!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of the recipe\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/overview.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If run `run.sh`, all of stages will be performed.\n",
    "\n",
    "But we can specify the stage to run with `--stage` options.\n",
    "\n",
    "- `run.sh --stage 0`: Run only the stage 0\n",
    "- `run.sh --stage 012`: Run the stages 0, 1, and 2.\n",
    "\n",
    "Here, let us run each stage step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 0: Data preparation\n",
    "\n",
    "This stage performs download of corpus and list preparation.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_0.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In arctic, there are seven speakers.  \n",
    "Here let us use `slt` to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# you can specify the speaker via --spk (default=slt)\n",
    "!./run.sh --stage 0 --spk slt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Corpus is saved in\n",
    "- `downloads/cmu_us_<spk_name>_arctic_mini`\n",
    "\n",
    "Two lists of wav files are created.\n",
    "- `data/tr_slt/wav.scp`: wav list file for training\n",
    "- `data/ev_slt/wav.scp`: wav list file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The list file is that:\n",
    "- Each line has the path of wav file\n",
    "- All of the lines are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    " !head -n 3 data/*_slt/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use 32 utts for training, 4 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l < data/tr_slt/wav.scp\n",
    "!wc -l < data/ev_slt/wav.scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 1: Feature extraction\n",
    "\n",
    "This stage performs feature extraction with the\n",
    "list file.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_1.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 1\n",
    "!head -n 36 run.sh | tail -n 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 1 with default settings\n",
    "!./run.sh --stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hyperparameters can be changed via command line.  \n",
    "But it will overwrite the existing ones. Be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example of changing hyperparameters of feature extraction\n",
    "# !./run.sh --stage 1 --mcep_dim 30 --shiftms 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Extracted features are saved as `hdf5` in\n",
    "- `hdf5/tr_slt/*.h5`: Feature file of training data \n",
    "- `hdf5/ev_slt/*.h5`: Feature file of evaluation data\n",
    "\n",
    "Lists of feature files are created \n",
    "- `data/tr_slt/feats.scp`\n",
    "- `data/ev_slt/feats.scp`\n",
    "\n",
    "High pass filtered training wav files are saved in\n",
    "- `wav_hpf/tr_slt/*.wav`: Filtered wav file of training data\n",
    "\n",
    "List of filetered wav files is created\n",
    "- `data/tr_slt/wav_hpf.scp`: List of filtered wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the list file format:\n",
    "- Each line has the path of feature or wav\n",
    "file  \n",
    "- All of the lines are sorted\n",
    "- Assume that all of the lists has the same\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 3 data/*_slt/feats.scp\n",
    "!echo \"\"\n",
    "!head -n 3 data/tr_slt/wav_hpf.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "hdf5 format can be loaded as `numpy.ndarray` in python using `h5py` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(\"hdf5/tr_slt/arctic_a0001.h5\") as f:\n",
    "    print(f.keys())\n",
    "    feat = f[\"world\"][()]\n",
    "# or you can use our utils\n",
    "from wavenet_vocoder.utils import read_hdf 5\n",
    "feat = read_hdf5(\"hdf5/tr_slt/arctic_a0001.h5\", \"world\")\n",
    "print(\"Feature shape: (#num_frames=%d, #feature_dims=%d)\" % (feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The feature is extracted with World.\n",
    "- `U/V binary` (1 dim)\n",
    "- `continuous F0` (1 dim), \n",
    "- `mcep`(25 dim = `mcep_dim + 1`) \n",
    "- `ap` (1 dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(feat[:, 0])\n",
    "plt.title(\"U/V binary\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(feat[:, 1])\n",
    "plt.title(\"Continuous F0\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(feat[:, 2:26].T, aspect=\"auto\")\n",
    "plt.title(\"Mel-cepstrum\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(feat[:, -1])\n",
    "plt.title(\"Aperiodicity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 2: Statistics calculation\n",
    "\n",
    "This stage calculates the mean and variance of features.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_2.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 2 with default settings\n",
    "!./run.sh --stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculated statistics are saved as `hdf5` format in\n",
    "- `data/tr_slt/stats.h5`\n",
    "\n",
    "`stats.h5` is used for:\n",
    "- Feature normalization during training\n",
    "- Calculation of noise shaping filter coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`stats.h5` can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
    "    print(f.keys())\n",
    "    print(f['world'].keys())\n",
    "    mean = f['world']['mean'][()]\n",
    "    scale = f['world']['scale'][()]\n",
    "    print(mean.shape)\n",
    "    print(scale.shape)\n",
    "    \n",
    "# or you use our utils\n",
    "mean = read_hdf5(\"data/tr_slt/stats.h5\", \"world/mean\")\n",
    "scale = read_hdf5(\"data/tr_slt/stats.h5\", \"world/scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 3: Noise weighting\n",
    "\n",
    "This stage applies noise weighting filter to training\n",
    "wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_3.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 3\n",
    "!head -n 38 run.sh | tail -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 3 with default settings\n",
    "!./run.sh --stage 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `use_noise_shaping=false`, `stage 3` will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Noise weighting filtered wav files are saved in\n",
    "- `wav_nwf/tr_slt/*.wav`\n",
    "\n",
    "The list of noise weighting filtered wav files is saved as\n",
    "- `data/tr_slt/wav_nwf.scp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*[0-9].h5|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the difference of waveform here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [],
   "source": [
    "# listen to the samples\n",
    "import IPython.display\n",
    "IPython.display.display(IPython.display.Audio(\"wav_hpf/tr_slt/arctic_a0001.wav\"))\n",
    "IPython.display.display(IPython.display.Audio(\"wav_nwf/tr_slt/arctic_a0001.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [],
   "source": [
    "# show spectrogram\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "x, fs = sf.read(\"wav_hpf/tr_slt/arctic_a0001.wav\")\n",
    "x_ns, fs = sf.read(\"wav_nwf/tr_slt/arctic_a0001.wav\")\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(x, Fs=fs)\n",
    "plt.title(\"Original spectrogram\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(x_ns, Fs=fs)\n",
    "plt.title(\"Noise weighting filtered spectrogram\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Filtering related parameters `mlas/coef` and `mlsa/alpha` are added in `data/tr_slt/stats.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "27"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
    "    print(f.keys())\n",
    "    print(f[\"mlsa\"].keys())\n",
    "    print(f[\"mlsa\"][\"alpha\"])\n",
    "    print(f[\"mlsa\"][\"coef\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mlsa/coef` is the coefficient of MLSA filter, which is calculated from averaged mel-cepstrum and `mag`.  \n",
    "`mlsa/alpha` is the hyperparameter `alpha`, all pass filter coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 4: WaveNet training\n",
    "\n",
    "This stage trains WaveNet using extracted\n",
    "features and noise weighting filtered wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_4.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 4\n",
    "!head -n 59 run.sh | tail -n 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 4 with default settings\n",
    "!./run.sh --stage 4 --iters 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Default network structure in `egs/arctic/sd-mini`.\n",
    "<div align=\"center\">\n",
    "    <img src=figs/wavenet.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example when `dilation_depth=3` and `dilation_repeat=2`.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/structure_ex.png width=45%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make a batch by split a waveform into pieces.\n",
    "- `batch_size`: Number of batches\n",
    "- `batch_length`: Length of each batch\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/batch.png width=65%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model parameters are saved as  \n",
    "- `exp/tr_arctic_16k_sd_world_slt_*/checkpoint-*.pkl` \n",
    "\n",
    "Modle configuration is saved as  \n",
    "- `exp/tr_arctic_16k_sd_world_slt_*/model.conf`\n",
    "\n",
    "The directory name is automatically set to be unique depending on hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|*.scp|*.log|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model configuration file can be loaded as `argparse.Namespace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "conf = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf\")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model parameters `checkpoint-*.pkl` can be loaded as `dict` which contains\n",
    "following information:\n",
    "- `iterations`: Number of iterations of this parameters\n",
    "- `optimizer`: `Dict` of states of optimizer\n",
    "- `model`: `OrderedDict` of Model\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl\")\n",
    "print(state_dict.keys())\n",
    "print(state_dict[\"iterations\"])\n",
    "print(state_dict[\"optimizer\"].keys())\n",
    "print(state_dict[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can resume training from `checkpoint-*.pkl` file with `--resume` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!./run.sh --stage 4 \\\n",
    "    --iters 1000 \\\n",
    "    --resume exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can train using multi-gpu with `--n_gpus` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab, we can use only a single gpu :(\n",
    "# batch_size must be >= n_gpus\n",
    "# !./run.sh --stage 4 --n_gpus 2 --batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 5: WaveNet decoding\n",
    "\n",
    "This stage performs decoding of evaluation data.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_5.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 5\n",
    "!head -n 69 run.sh | tail -n 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 5 with default setting\n",
    "!./run.sh --stage 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can specify the `checkpoint-*.pkl` file used for decoding and directory to\n",
    "be saved via `--checkpoint` and `--outdir` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes times, comment out\n",
    "# !./run.sh --stage 5 \\\n",
    "#     --checkpoint exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-100.pkl\n",
    "#     --outdir exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/wav_ckpt_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use multi-gpu decoding via `--n_gpus` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab, we can use only a single gpu :(\n",
    "# !./run.sh --stage 5 --n_gpus 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated wav files are saved in \n",
    "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "36"
    }
   },
   "outputs": [],
   "source": [
    "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 6: Noise shaping\n",
    "\n",
    "This stage applies noise shaping filter to generated wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_6.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "37"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 6 with default setting\n",
    "!./run.sh --stage 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Restored wav files are saved in\n",
    "\n",
    "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav_nsf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "38"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finished! Unfortunately, generated samples are just-like a noise.  \n",
    "So Let us check the samples which trained with `egs/arctic/sd` from  \n",
    "https://kan-bayashi.github.io/WaveNetVocoderSamples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Use pretrained model as vocoder\n",
    "\n",
    "Here we show how-to-use pretrained model as\n",
    "vocoder.  \n",
    "What we need to prepare is following three files:\n",
    "\n",
    "- `model.conf`:\n",
    "Model configuration file.\n",
    "- `checkpoint-*.pkl`: Model parameter file.\n",
    "- `stats.h5`: Statistics file.\n",
    "\n",
    "Let us pack following files into\n",
    "`pretrained_model/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "39"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# summarize trained model in the directory\n",
    "!mkdir pretrained_model\n",
    "!cp -v exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/stats.h5 \\\n",
    "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf \\\n",
    "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-1000.pkl \\\n",
    "    pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, please prepare the list file of feature files to be decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "40"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# here make a dummy features and the stored as hdf5 with key \"/world\"\n",
    "os.makedirs(\"dummy\", exist_ok=True)\n",
    "for idx, n_frames in enumerate([10, 20, 30, 40]): \n",
    "    x = np.random.randn(n_frames, 28)  # (#num_frames, #feature_dims)\n",
    "    with h5py.File(\"dummy/dummy_%d.h5\" % idx, \"w\") as f:\n",
    "        f[\"world\"] = x\n",
    "\n",
    "# make a list of features to be decoded.\n",
    "!find dummy -name \"*.h5\" > dummy_feats.scp\n",
    "\n",
    "# check\n",
    "!cat dummy_feats.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Run the `--stage 56` by specifying `--feats` in the recipe directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "41"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# decode with pretrained model through the recipe\n",
    "!./run.sh --stage 56 \\\n",
    "    --outdir dummy_feats_wav \\\n",
    "    --feats dummy_feats.scp \\\n",
    "    --checkpoint pretrained_model/checkpoint-1000.pkl\n",
    "!ls dummy_feats_wav*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you want to use outside of the recipe, directly call python scripts stored in\n",
    "`wavenet_vocoder/bin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "42"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# decode with pretrained model\n",
    "!python ../../../wavenet_vocoder/bin/decode.py \\\n",
    "     --feats dummy_feats.scp \\\n",
    "     --outdir dummy_feats_wav_2 \\\n",
    "     --checkpoint pretrained_model/checkpoint-1000.pkl \\\n",
    "     --fs 16000 \\\n",
    "     --n_gpus 1 \\\n",
    "     --batch_size 4\n",
    "# make list of wav files to be filtered\n",
    "!find dummy_feats_wav_2 -name \"*.wav\" > dummy_feats_wav_2/wav.scp\n",
    "# apply noise shaping filter\n",
    "!python ../../../wavenet_vocoder/bin/noise_shaping.py \\\n",
    "     --waveforms dummy_feats_wav_2/wav.scp \\\n",
    "     --outdir dummy_feats_wav_2_nsf \\\n",
    "     --stats pretrained_model/stats.h5 \\\n",
    "     --fs 16000 \\\n",
    "     --shiftms 5\n",
    "!ls dummy_feats_wav_2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combine with Sprocket\n",
    "\n",
    "Let us show how-to-combine wavenet vocoder with voice conversion toolkit [sprocket](https://github.com/k2kobayashi/sprocket).    \n",
    "Here, we generate converted voice with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed directory\n",
    "!mkdir ../../../../conversion_example\n",
    "os.chdir(\"../../../../conversion_example\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, download pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# download sprocket model\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1PiGDyYDQt0b4h6KAV1MOmDxHjHUv1cT6\" \\\n",
    "    downloads/sprocket_pretrained\n",
    "\n",
    "# download wavenet vocoder model\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1AhtRB0vTkjDrum-dfgaiXnQgsAAiYMGW\" \\\n",
    "    downloads/wavenet_vocoder_pretrained\n",
    "\n",
    "# download wav samples\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1kBwF7ejyCR5aI9FitmMSCnWdPCNVouqg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sprocket pretrained model\n",
    "    - `GMM_mcep.pkl`: GMM param file for mcep conversion.\n",
    "    - `<src_spk>.yml`: Source speaker yaml file.\n",
    "    - `<src_spk>-<tar_spk>.yml`: Source-target speaker pair yaml file.\n",
    "    - `<src_spk>.h5`: Statistics file of source speaker.\n",
    "    - `<tar_spk>.h5`: Statistics file of target speaker.\n",
    "    - `cvgv.h5`: Statistics file of global variance for converted features.\n",
    "    \n",
    "- Target speaker WaveNet vocoder pretrained model\n",
    "    - `model.conf`: Model configuration file.\n",
    "    - `checkpoint-*.pkl`: Model parameter file.\n",
    "    - `stats.h5`: Statistics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls downloads/*pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, extract features and then convert them to target speaker's one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "![ ! -e hdf5 ] && mkdir hdf5\n",
    "![ ! -e wav ] && mkdir wav\n",
    "!PYTHONPATH=../sprocket/example/src \\\n",
    "    python ../sprocket/sprocket/bin/convert_feats.py \\\n",
    "        --cvmcep0th True \\\n",
    "        --cvcodeap True \\\n",
    "        --cvgvstats downloads/sprocket_pretrained/cvgv.h5 \\\n",
    "        --org_yml downloads/sprocket_pretrained/rms.yml \\\n",
    "        --pair_yml downloads/sprocket_pretrained/rms-slt.yml \\\n",
    "        --org_stats downloads/sprocket_pretrained/rms.h5 \\\n",
    "        --tar_stats downloads/sprocket_pretrained/slt.h5 \\\n",
    "        --mcepgmmf downloads/sprocket_pretrained/GMM_mcep.pkl \\\n",
    "        --iwav downloads/samples/src/arctic_b0536.wav \\\n",
    "        --cvfeats hdf5/arctic_b0536.h5 \\\n",
    "        --owav wav/arctic_b0536.wav\n",
    "!ls hdf5 wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then generate waveform with pretrained wavenet using converted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: require too much time.\n",
    "# decode with wavenet vocoder\n",
    "!find hdf5 -name \"*.h5\" > hdf5/feats.scp\n",
    "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/decode.py \\\n",
    "     --feats hdf5/feats.scp \\\n",
    "     --outdir wav_wnv \\\n",
    "     --checkpoint downloads/wavenet_vocoder_pretrained/checkpoint-final.pkl \\\n",
    "     --fs 16000 \\\n",
    "     --n_gpus 1 \\\n",
    "     --batch_size 4\n",
    "# apply noise shaping filter\n",
    "!find wav_wnv -name \"*.wav\" > wav_wnv/wav.scp\n",
    "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/noise_shaping.py \\\n",
    "     --waveforms wav_wnv/wav.scp \\\n",
    "     --outdir wav_wnv_nsf \\\n",
    "     --stats downloads/wavenet_vocoder_pretrained/stats.h5 \\\n",
    "     --fs 16000 \\\n",
    "     --shiftms 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# listen to pre-synthesized ones\n",
    "import IPython.display\n",
    "print(\"Source\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/src/arctic_b0536.wav\"))\n",
    "print(\"Target\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/tar/arctic_b0536.wav\"))\n",
    "print(\"Converted voice with vocoder\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/vocoder/arctic_b0536.wav\"))\n",
    "print(\"Converted voice with wavenet vocoder\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/wavenet_vocoder/arctic_b0536.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"running time = %s minite\" % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Introduced voice conversion with direct waveform modeling\n",
    "- Introduced Sprocket /  PytorchWaveNetVocoder\n",
    "    - Can build GMM-based VC / DIFFVC  & WaveNet vocoder\n",
    "    - Can combine both module to generate high quality converted voices\n",
    "\n",
    "Thank you for your attendance!  \n",
    "If you have time, please send us feedback via [Google form](https://forms.gle/28QrvGRBAAiKpWas8). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "wavenet_vocoder.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
